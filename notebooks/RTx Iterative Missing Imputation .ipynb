{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfef9703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install requests\n",
    "# !pip install tabulate\n",
    "# !pip install future\n",
    "# !pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9edc171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 17.0.2+8-LTS-86, mixed mode, sharing)\n",
      "  Starting server from C:\\Users\\charl\\anaconda3\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\charl\\AppData\\Local\\Temp\\tmp31z15k7i\n",
      "  JVM stdout: C:\\Users\\charl\\AppData\\Local\\Temp\\tmp31z15k7i\\h2o_charl_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\charl\\AppData\\Local\\Temp\\tmp31z15k7i\\h2o_charl_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Vancouver</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>1 month and 15 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_charl_1uat71</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>7.906 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.9.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         02 secs\n",
       "H2O_cluster_timezone:       America/Vancouver\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.0.2\n",
       "H2O_cluster_version_age:    1 month and 15 days\n",
       "H2O_cluster_name:           H2O_from_python_charl_1uat71\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    7.906 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.9.7 final\n",
       "--------------------------  -------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "import pandas as pd\n",
    "from h2o.frame import H2OFrame\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import os, time\n",
    "# h2o.cluster().shutdown() # Uncomment if need to restart cluster\n",
    "h2o.init() # Start Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19a78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "#     'record_id': 'string',\n",
    "    'susp_LN_prsnt_composite': 'enum',\n",
    "    'susp_LN_size_composite': 'real',\n",
    " 'dob': 'time',\n",
    " 'age_at_dx': 'real',\n",
    " 'biop_to_dx': 'real',\n",
    " 'surg_to_dx': 'real',\n",
    " 'men_status': 'enum',\n",
    " 'fmhx': 'int',\n",
    " 'gensus___1': 'enum',\n",
    " 'gensus___2': 'enum',\n",
    " 'gensus___3': 'enum',\n",
    " 'gensus___4': 'enum',\n",
    " 'gensus___5': 'enum',\n",
    " 'gensus___6': 'enum',\n",
    " 'gensus___7': 'enum',\n",
    " 'systhe___1': 'enum',\n",
    " 'systhe___2': 'enum',\n",
    " 'systhe___3': 'enum',\n",
    " 'systhe___4': 'enum',\n",
    " 'systhe___5': 'enum',\n",
    " 'tumor_laterality': 'enum',\n",
    " 'height_cm': 'real',\n",
    " 'weight_kg': 'real',\n",
    " 'bra_cup_size': 'enum',\n",
    " 'bra_cup_size_measure': 'real',\n",
    " 'palpability': 'enum',\n",
    " 'axillary_lymph_node_palpab': 'enum',\n",
    " 'dximg___1': 'enum',\n",
    " 'dximg___2': 'enum',\n",
    " 'dximg___3': 'enum',\n",
    " 'dximg_date': 'time',\n",
    " 'img_size': 'real',\n",
    " 'foci': 'enum',\n",
    " 'tumor_stge': 'enum',\n",
    " 'susp_LN_presnt_composite': 'enum',\n",
    " 'lymph_node_max_size_mm': 'real',\n",
    " 'tumor_size_mm': 'real',\n",
    " 'microcalcifications': 'enum',\n",
    " 'extent_of_calcification_ma': 'enum',\n",
    " 'prominent_axillary_lymph': 'enum',\n",
    " 'lymph_node_max_size_mm0': 'real',\n",
    " 'backgroun_enhancement': 'enum',\n",
    " 'max_enhancement_measurment': 'real',\n",
    " 'axillary_lymphadenopathy': 'enum',\n",
    " 'internal_mammary_lymphaden': 'enum',\n",
    " 'high_grade_fdg_foci_presen': 'enum',\n",
    " 'size_of_the_largest_foci_c': 'real',\n",
    " 'axillary_lymphadenopathy_p': 'enum',\n",
    " 'axillary_lymph_node_max_si': 'enum',\n",
    " 'int_mammary_lymphade_pet': 'real',\n",
    " 'internal_mammary_lymph_nod': 'real',\n",
    " 'pre_op_biop_date': 'time',\n",
    " 'pre_op_biopsy': 'enum',\n",
    " 'tumor_location': 'enum',\n",
    " 'tumor_location_trans': 'int',\n",
    " 'his_subtype___1': 'enum',\n",
    " 'his_subtype___2': 'enum',\n",
    " 'his_subtype___3': 'enum',\n",
    " 'his_subtype___4': 'enum',\n",
    " 'his_subtype___5': 'enum',\n",
    " 'his_subtype___6': 'enum',\n",
    " 'specify_histology_if_non_o': 'enum',\n",
    " 'tumor_grade': 'enum',\n",
    " 'margin_status': 'enum',\n",
    " 'closest_margin': 'enum',\n",
    " 'closest_margin_trans': 'enum',\n",
    " 'distance_from_closest_marg': 'real',\n",
    " 'lymphovascular_invasion0': 'enum',\n",
    " 'er_status': 'enum',\n",
    " 'pr_status': 'enum',\n",
    " 'her_status': 'enum',\n",
    " 'imaging_and_biopsy_concord': 'enum',\n",
    " 'axillary_lymph_node_core_b': 'enum',\n",
    " 'metastatic_carcinoma_on_ax': 'enum',\n",
    " 'surgical_indication1_primary_treatment___1': 'enum',\n",
    " 'surgical_indication1_primary_treatment___2': 'enum',\n",
    " 'surgical_indication1_primary_treatment___3': 'enum',\n",
    " 'surgical_indication1_primary_treatment___4': 'enum',\n",
    " 'surgical_indication1_primary_treatment___5': 'enum',\n",
    " 'laterality': 'enum',\n",
    " 'surgery_date': 'time',\n",
    " 'breast_procedure': 'enum',\n",
    " 'axillary_surgery___1': 'enum',\n",
    " 'axillary_surgery___2': 'enum',\n",
    " 'axillary_surgery___3': 'enum',\n",
    " 'lymph_nodes': 'int',\n",
    " 'sln': 'int',\n",
    " 'number_of_positive_sln': 'int',\n",
    " 'ax_nodes': 'int',\n",
    " 'mastectomy_weight_g': 'real',\n",
    " 'tumor_size': 'real',\n",
    " 'tumor_loc': 'enum',\n",
    " 'his_type___1': 'enum',\n",
    " 'his_type___2': 'enum',\n",
    " 'his_type___3': 'enum',\n",
    " 'his_type___4': 'enum',\n",
    " 'his_type___5': 'enum',\n",
    " 'his_type___6': 'enum',\n",
    " 'his_type___7': 'enum',\n",
    " 'his_type___8': 'enum',\n",
    " 'specify_histology_if_other': 'enum',\n",
    " 'tu_grade': 'enum',\n",
    " 'tumor_focality': 'enum',\n",
    " 'num_foci': 'real',\n",
    " 'lymphovascular_invasion': 'enum',\n",
    " 'in_situ_component_present': 'enum',\n",
    " 'in_situ_component_type___1': 'enum',\n",
    " 'in_situ_component_type___2': 'enum',\n",
    " 'in_situ_component_size_mm': 'real',\n",
    " 'in_situ_component_grade': 'enum',\n",
    " 'mar_status': 'enum',\n",
    " 'clos_margin___1': 'enum',\n",
    " 'clos_margin___2': 'enum',\n",
    " 'clos_margin___3': 'enum',\n",
    " 'clos_margin___4': 'enum',\n",
    " 'clos_margin___5': 'enum',\n",
    " 'clos_margin___6': 'enum',\n",
    " 'clos_margin___7': 'enum',\n",
    " 'dis_closest_margin': 'real',\n",
    " 'metastasis': 'enum',\n",
    " 'ln_w_micrometastasis': 'real',\n",
    " 'ln_w_macrometastasis': 'real',\n",
    " 'size_of_largest_nodal_meta': 'real',\n",
    " 'extranodal_extension': 'enum',\n",
    " 'extent_of_extranodal_exten': 'real',\n",
    " 'did_the_patient_receive_pm': 'enum',\n",
    "#  'did_the_patient_reject_pmr': 'int',\n",
    "#  'data_collection_fields_complete': 'int'\n",
    "            }\n",
    "\n",
    "# Helper to print in terminal with colors\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "def my_print(*args, add_sep=False):\n",
    "    # print with orange\n",
    "    text = \" \".join(args)\n",
    "    if add_sep:\n",
    "        text = \"-\"*50+\"\\n\"+text+\"\\n\"+\"-\"*50\n",
    "    print(bcolors.WARNING, text, bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "696ddbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_checkpoint = True\n",
    "df_path = r\"D:\\\\YifuChen\\\\Jan20-2022\\\\H2ODAI\\\\dai-1.10.1\\\\data\\\\ALLTran.csv\"\n",
    "# To restore a checkpoint, use a path like below\n",
    "df_checkpoint_path = r\"D:\\YifuChen\\Jan20-2022\\H2ODAI\\dai-1.10.1\\data\\Imputed\\ALLTranImputed_epoch0_180sec_PREonly_92_2022-03-11-19_10_10.csv.\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a06b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-operative columns, available during prediction\n",
    "pre_cols = ['susp_LN_prsnt_composite', \"susp_LN_size_composite\", 'dob', 'age_at_dx', 'biop_to_dx', 'surg_to_dx', 'men_status', 'fmhx', 'gensus___1', 'gensus___2', 'gensus___3', 'gensus___4', 'gensus___5', 'gensus___6', 'gensus___7', 'systhe___1', 'systhe___2', 'systhe___3', 'systhe___4', 'systhe___5', 'tumor_laterality', 'height_cm', 'weight_kg', 'bra_cup_size', 'bra_cup_size_measure', 'palpability', 'axillary_lymph_node_palpab', 'dximg___1', 'dximg___2', 'dximg___3', 'dximg_date', 'img_size', 'foci', 'tumor_stge', 'susp_LN_presnt_composite', 'lymph_node_max_size_mm', 'tumor_size_mm', 'microcalcifications', 'extent_of_calcification_ma', 'prominent_axillary_lymph', 'lymph_node_max_size_mm0', 'backgroun_enhancement', 'max_enhancement_measurment', 'axillary_lymphadenopathy', 'internal_mammary_lymphaden', 'high_grade_fdg_foci_presen', 'size_of_the_largest_foci_c', 'axillary_lymphadenopathy_p', 'axillary_lymph_node_max_si', 'int_mammary_lymphade_pet', 'internal_mammary_lymph_nod', 'pre_op_biop_date', 'pre_op_biopsy', 'tumor_location', 'tumor_location_trans', 'his_subtype___1', 'his_subtype___2', 'his_subtype___3', 'his_subtype___4', 'his_subtype___5', 'his_subtype___6', 'specify_histology_if_non_o', 'tumor_grade', 'margin_status', 'closest_margin', 'closest_margin_trans', 'distance_from_closest_marg', 'lymphovascular_invasion0', 'er_status', 'pr_status', 'her_status', 'imaging_and_biopsy_concord', 'axillary_lymph_node_core_b', 'metastatic_carcinoma_on_ax']\n",
    "# Intra-operative columns, may be available during prediction\n",
    "intra_cols = ['surgical_indication1_primary_treatment___1', 'surgical_indication1_primary_treatment___2', 'surgical_indication1_primary_treatment___3', 'surgical_indication1_primary_treatment___4', 'surgical_indication1_primary_treatment___5', 'laterality', 'surgery_date', 'breast_procedure', 'axillary_surgery___1', 'axillary_surgery___2', 'axillary_surgery___3', 'lymph_nodes', 'sln']\n",
    "# Post-operative columns, not available during prediction\n",
    "post_cols = ['number_of_positive_sln', 'ax_nodes', 'mastectomy_weight_g', 'tumor_size', 'tumor_loc', 'his_type___1', 'his_type___2', 'his_type___3', 'his_type___4', 'his_type___5', 'his_type___6', 'his_type___7', 'his_type___8', 'specify_histology_if_other', 'tu_grade', 'tumor_focality', 'num_foci', 'lymphovascular_invasion', 'in_situ_component_present', 'in_situ_component_type___1', 'in_situ_component_type___2', 'in_situ_component_size_mm', 'in_situ_component_grade', 'mar_status', 'clos_margin___1', 'clos_margin___2', 'clos_margin___3', 'clos_margin___4', 'clos_margin___5', 'clos_margin___6', 'clos_margin___7', 'dis_closest_margin', 'metastasis', 'ln_w_micrometastasis', 'ln_w_macrometastasis', 'size_of_largest_nodal_meta', 'extranodal_extension', 'extent_of_extranodal_exten', 'did_the_patient_receive_pm']\n",
    "# 15 Most important columns, evaluated by Shapely's value or feature importance\n",
    "important_cols = [\n",
    "    'age_at_dx',\n",
    "    'ax_nodes',\n",
    "    'his_subtype___3',\n",
    "    'systhe___1',\n",
    "    'systhe___5',\n",
    "    'mar_status',\n",
    "    'metastatic_carcinoma_on_ax',\n",
    "    'biop_to_dx',\n",
    "    'tumor_size',\n",
    "    'num_foci',\n",
    "    'size_of_the_largest_foci_c',\n",
    "    'int_mammary_lymphade_pet',\n",
    "    'clos_margin___2',\n",
    "    'axillary_surgery___2',\n",
    "    'axillary_surgery___3'\n",
    "]\n",
    "2\n",
    "abnormal_ln_cols = [\"susp_LN_presnt_composite\", \"prominent_axillary_lymph\", \"axillary_lymphadenopathy\", \"internal_mammary_lymphaden\", \"axillary_lymphadenopathy_p\", \"int_mammary_lymphade_pet\"]\n",
    "susp_LN_size_composite_cols = [\"lymph_node_max_size_mm\", \"lymph_node_max_size_mm0\", \"axillary_lymph_node_max_si\", \"internal_mammary_lymph_nod\"]\n",
    "imputation_dict = {\n",
    "    \"bi_rads_score\": \"\",\n",
    "    \"tumor_stge\": \"\",\n",
    "    \"susp_LN_presnt_composite\": 2,\n",
    "    \"lymph_node_max_size_mm\": 0,\n",
    "    \"extent_of_calcification_ma\": 0,\n",
    "    \"prominent_axillary_lymph\": 2,\n",
    "    \"backgroun_enhancement\": 2,\n",
    "    \"max_enhancement_measurement\": 0,\n",
    "    \"axillary_lymphadenopathy\": 2,\n",
    "    \"internal_mammary_lymphaden\": 2,\n",
    "    \"high_grade_fdg_foci_presen\": \"\",\n",
    "    \"size_of_the_largest_foci_c\": \"\",\n",
    "    \"axillary_lymphadenopathy_p\": 0,\n",
    "#     \"axillary_lymph_node_max_si\"\n",
    "    \"internal_mammry_lymph_nod\": 0,\n",
    "    \"er_status\": 0.5,\n",
    "    \"pr_status\": 0.5,\n",
    "    \"her_status\": 0.5,\n",
    "    \"axillary_lymph_node_core_b\": 0,\n",
    "}\n",
    "\n",
    "        \n",
    "# Preprocess columns by constructing new columns (feature engineering)\n",
    "if \"susp_LN_size_composite\" not in df.columns:\n",
    "    susp_LN_size_composites = []\n",
    "    for i, row in df.iterrows():\n",
    "        max_size = 0\n",
    "        for col in susp_LN_size_composite_cols:\n",
    "            value = row[col]\n",
    "            if str(value) == \"nan\":\n",
    "                continue\n",
    "            max_size = max(max_size, value)\n",
    "        susp_LN_size_composites.append(str(max_size))\n",
    "    df.insert(10, \"susp_LN_size_composite\", susp_LN_size_composites)\n",
    "\n",
    "if \"susp_LN_prsnt_composite\" not in df.columns:\n",
    "    abnormal_ln = []\n",
    "    for i, row in df.iterrows():\n",
    "        cur = \"0\"\n",
    "        for col in abnormal_ln_cols:\n",
    "            value = row[col]\n",
    "            if str(value) == \"nan\":\n",
    "                continue\n",
    "            else:\n",
    "                if str(value).strip().replace(\".0\",\"\")  == \"1\":\n",
    "                    cur = \"1\"\n",
    "        abnormal_ln.append(cur)\n",
    "    df.insert(11, \"susp_LN_prsnt_composite\",abnormal_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0495995",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking the following columns:\n",
      " ['susp_LN_prsnt_composite', 'susp_LN_size_composite', 'dob', 'age_at_dx', 'biop_to_dx', 'surg_to_dx', 'men_status', 'fmhx', 'gensus___1', 'gensus___2', 'gensus___3', 'gensus___4', 'gensus___5', 'gensus___6', 'gensus___7', 'systhe___1', 'systhe___2', 'systhe___3', 'systhe___4', 'systhe___5', 'tumor_laterality', 'height_cm', 'weight_kg', 'bra_cup_size', 'bra_cup_size_measure', 'palpability', 'axillary_lymph_node_palpab', 'dximg___1', 'dximg___2', 'dximg___3', 'dximg_date', 'img_size', 'foci', 'tumor_stge', 'susp_LN_presnt_composite', 'lymph_node_max_size_mm', 'tumor_size_mm', 'microcalcifications', 'extent_of_calcification_ma', 'prominent_axillary_lymph', 'lymph_node_max_size_mm0', 'backgroun_enhancement', 'max_enhancement_measurment', 'axillary_lymphadenopathy', 'internal_mammary_lymphaden', 'high_grade_fdg_foci_presen', 'size_of_the_largest_foci_c', 'axillary_lymphadenopathy_p', 'axillary_lymph_node_max_si', 'int_mammary_lymphade_pet', 'internal_mammary_lymph_nod', 'pre_op_biop_date', 'pre_op_biopsy', 'tumor_location', 'tumor_location_trans', 'his_subtype___1', 'his_subtype___2', 'his_subtype___3', 'his_subtype___4', 'his_subtype___5', 'his_subtype___6', 'specify_histology_if_non_o', 'tumor_grade', 'margin_status', 'closest_margin', 'closest_margin_trans', 'distance_from_closest_marg', 'lymphovascular_invasion0', 'er_status', 'pr_status', 'her_status', 'imaging_and_biopsy_concord', 'axillary_lymph_node_core_b', 'metastatic_carcinoma_on_ax'] \n",
      "\n",
      "\n",
      "Checking the following columns:\n",
      " ['surgical_indication1_primary_treatment___1', 'surgical_indication1_primary_treatment___2', 'surgical_indication1_primary_treatment___3', 'surgical_indication1_primary_treatment___4', 'surgical_indication1_primary_treatment___5', 'laterality', 'surgery_date', 'breast_procedure', 'axillary_surgery___1', 'axillary_surgery___2', 'axillary_surgery___3', 'lymph_nodes', 'sln'] \n",
      "\n",
      "\n",
      "Checking the following columns:\n",
      " ['number_of_positive_sln', 'ax_nodes', 'mastectomy_weight_g', 'tumor_size', 'tumor_loc', 'his_type___1', 'his_type___2', 'his_type___3', 'his_type___4', 'his_type___5', 'his_type___6', 'his_type___7', 'his_type___8', 'specify_histology_if_other', 'tu_grade', 'tumor_focality', 'num_foci', 'lymphovascular_invasion', 'in_situ_component_present', 'in_situ_component_type___1', 'in_situ_component_type___2', 'in_situ_component_size_mm', 'in_situ_component_grade', 'mar_status', 'clos_margin___1', 'clos_margin___2', 'clos_margin___3', 'clos_margin___4', 'clos_margin___5', 'clos_margin___6', 'clos_margin___7', 'dis_closest_margin', 'metastasis', 'ln_w_micrometastasis', 'ln_w_macrometastasis', 'size_of_largest_nodal_meta', 'extranodal_extension', 'extent_of_extranodal_exten', 'did_the_patient_receive_pm'] \n",
      "\n",
      "\n",
      "Checking the following columns:\n",
      " ['age_at_dx', 'ax_nodes', 'his_subtype___3', 'systhe___1', 'systhe___5', 'mar_status', 'metastatic_carcinoma_on_ax', 'biop_to_dx', 'tumor_size', 'num_foci', 'size_of_the_largest_foci_c', 'int_mammary_lymphade_pet', 'clos_margin___2', 'axillary_surgery___2', 'axillary_surgery___3'] \n",
      "\n",
      "\u001b[93m DataFrame head: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Local\\Temp/ipykernel_17808/1248690902.py:10: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df = df.drop(col, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob</th>\n",
       "      <th>age_at_dx</th>\n",
       "      <th>biop_to_dx</th>\n",
       "      <th>surg_to_dx</th>\n",
       "      <th>men_status</th>\n",
       "      <th>fmhx</th>\n",
       "      <th>gensus___1</th>\n",
       "      <th>gensus___2</th>\n",
       "      <th>gensus___3</th>\n",
       "      <th>susp_LN_size_composite</th>\n",
       "      <th>...</th>\n",
       "      <th>clos_margin___6</th>\n",
       "      <th>clos_margin___7</th>\n",
       "      <th>dis_closest_margin</th>\n",
       "      <th>metastasis</th>\n",
       "      <th>ln_w_micrometastasis</th>\n",
       "      <th>ln_w_macrometastasis</th>\n",
       "      <th>size_of_largest_nodal_meta</th>\n",
       "      <th>extranodal_extension</th>\n",
       "      <th>extent_of_extranodal_exten</th>\n",
       "      <th>did_the_patient_receive_pm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-10-01</td>\n",
       "      <td>57.372603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1966-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962-06-01</td>\n",
       "      <td>55.843836</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1960-11-01</td>\n",
       "      <td>57.975342</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1959-10-01</td>\n",
       "      <td>55.153425</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>7.966667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dob  age_at_dx  biop_to_dx  surg_to_dx  men_status  fmhx  \\\n",
       "0  1960-10-01  57.372603         NaN    8.000000         2.0   0.0   \n",
       "1  1966-01-01        NaN         NaN         NaN         2.0   0.0   \n",
       "2  1962-06-01  55.843836    3.466667         NaN         2.0   0.0   \n",
       "3  1960-11-01  57.975342    0.233333    1.900000         2.0   1.0   \n",
       "4  1959-10-01  55.153425    0.433333    7.966667         2.0   0.0   \n",
       "\n",
       "   gensus___1  gensus___2  gensus___3 susp_LN_size_composite  ... clos_margin___6  \\\n",
       "0           0           0           0             31.0  ...               0   \n",
       "1           0           0           0                0  ...               0   \n",
       "2           0           0           0                0  ...               0   \n",
       "3           1           0           0                0  ...               0   \n",
       "4           0           0           0                0  ...               0   \n",
       "\n",
       "   clos_margin___7  dis_closest_margin  metastasis  ln_w_micrometastasis  \\\n",
       "0                0                 NaN         NaN                   NaN   \n",
       "1                0                  20         2.0                   NaN   \n",
       "2                0                   1         2.0                   NaN   \n",
       "3                0                   3         2.0                   NaN   \n",
       "4                0                 NaN         1.0                   0.0   \n",
       "\n",
       "   ln_w_macrometastasis  size_of_largest_nodal_meta  extranodal_extension  \\\n",
       "0                   NaN                         NaN                   NaN   \n",
       "1                   NaN                         NaN                   2.0   \n",
       "2                   NaN                         NaN                   2.0   \n",
       "3                   NaN                         NaN                   2.0   \n",
       "4                   1.0                           5                   1.0   \n",
       "\n",
       "   extent_of_extranodal_exten  did_the_patient_receive_pm  \n",
       "0                         NaN                         0.0  \n",
       "1                         NaN                         0.0  \n",
       "2                         NaN                         0.0  \n",
       "3                         NaN                         0.0  \n",
       "4                         NaN                         1.0  \n",
       "\n",
       "[5 rows x 126 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for cols in [pre_cols, intra_cols, post_cols, important_cols]:\n",
    "    print(\"\\nChecking the following columns:\\n\", cols,\"\\n\")\n",
    "    assert len(set(cols)) == len(cols) # Check no duplicates\n",
    "    assert set(cols).issubset(df.columns), f\"{set(cols) - set(df.columns)} columns do not exist in DataFrame\"# Check exists in DF\n",
    "\n",
    "# Drop irrelevant columns\n",
    "drop_cols = ['record_id', 'did_the_patient_reject_pmr', 'data_collection_fields_complete']\n",
    "for col in drop_cols:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(col, 1)\n",
    "\n",
    "\n",
    "# Convert applicable columns to factor type\n",
    "# factor_cols = [\"did_the_patient_receive_pm\", \"men_status\", \"tumor_laterality\", \"tumor_stge\", 'susp_LN_presnt_composite', \"backgroun_enhancement\", \"pre_op_biopsy\", \"tumor_grade\", \"margin_status\", \"er_status\", \"pr_status\", \"her_status\", \"imaging_and_biopsy_concord\", \"metastatic_carcinoma_on_ax\", \"laterality\", \"breast_procedure\", 'gensus___1', 'gensus___2', 'gensus___3', 'gensus___4', 'gensus___5', 'gensus___6', 'gensus___7', 'systhe___1', 'systhe___2', 'systhe___3', 'systhe___4', 'systhe___5', 'dximg___1', 'dximg___2', 'dximg___3', 'his_subtype___1', 'his_subtype___2', 'his_subtype___3', 'his_subtype___4', 'his_subtype___5', 'his_subtype___6', 'surgical_indication1_primary_treatment___1', 'surgical_indication1_primary_treatment___2', 'surgical_indication1_primary_treatment___3', 'surgical_indication1_primary_treatment___4', 'surgical_indication1_primary_treatment___5']\n",
    "# for col in factor_cols:\n",
    "#     df[col] = df[col].astype('category')# .asfactor()\n",
    "\n",
    "df_all = pd.DataFrame(df)\n",
    "df_pre = df[pre_cols]\n",
    "df_intra = df[intra_cols]\n",
    "df_post = df[post_cols]\n",
    "\n",
    "my_print(\"DataFrame head:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e58978d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = defaultdict(dict)\n",
    "models_pre_only = defaultdict(dict)\n",
    "cache = defaultdict(dict)\n",
    "cache_pre_only = defaultdict(dict)\n",
    "num_filled = 0\n",
    "skipped_cols = set()\n",
    "leaderboards = {}  # Key is: (epoch_num, y_col)\n",
    "missing_cells = {} # Key is: (row_idx, y_col), which refers to a cell in dataframe\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fcd00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pre_only = defaultdict(dict)\n",
    "cache_pre_only = defaultdict(dict)\n",
    "# df_pre_only = train.as_data_frame(use_pandas=True) # Missing values of ALL columns were filled by imputing based on PRE only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9fd2d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# temp_train = H2OFrame(df[[\"susp_LN_prsnt_composite\", \"did_the_patient_receive_pm\"]],column_types={\"susp_LN_prsnt_composite\":\"enum\", \"did_the_patient_receive_pm\":\"enum\"})\n",
    "# print(temp_train.columns)\n",
    "# temp_model = H2OAutoML(max_runtime_secs=60,seed=1)\n",
    "# temp_model.train(x=[\"susp_LN_prsnt_composite\"], y=\"did_the_patient_receive_pm\", training_frame=temp_train)\n",
    "# print(temp_model.leaderboard.head(rows=temp_model.leaderboard.nrows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60461b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An experiment from Alan's suggestion\n",
    "output_dir = r\"D:\\YifuChen\\Jan20-2022\\H2ODAI\\dai-1.10.1\\data\\Imputed\"\n",
    "pickle_dir = r\"D:\\YifuChen\\Jan20-2022\\H2ODAI\\dai-1.10.1\\data\\Cache\"\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d-%H_%M_%S\")\n",
    "temp_path = os.path.join(output_dir, fr\"AllImputed_abnormalLN.csv\")\n",
    "# df3 = df[[\"susp_LN_prsnt_composite\", \"did_the_patient_receive_pm\"]]\n",
    "# df3.dropna(subset = [\"did_the_patient_receive_pm\"], inplace=True)\n",
    "df.to_csv(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bbebeb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m dob column has type Time, hence skipped. \u001b[0m\n",
      "\u001b[93m gensus___1 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m gensus___2 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m gensus___3 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m susp_LN_size_composite column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m susp_LN_prsnt_composite column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m gensus___4 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m gensus___5 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m gensus___6 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m gensus___7 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m systhe___1 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m systhe___2 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m systhe___3 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m systhe___4 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m systhe___5 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m dximg___1 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m dximg___2 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m dximg___3 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m his_subtype___1 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m his_subtype___2 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m his_subtype___3 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m his_subtype___4 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m his_subtype___5 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m his_subtype___6 column has no missing values, hence skipped. \u001b[0m\n",
      "\u001b[93m dximg_date column has type Time, hence skipped. \u001b[0m\n",
      "\u001b[93m pre_op_biop_date column has type Time, hence skipped. \u001b[0m\n",
      "\u001b[93m axillary_lymph_node_max_si column has sparsity (missing) of 0.9525, missing percentage above threshold of 0.95, hence skipped \u001b[0m\n",
      "\u001b[93m distance_from_closest_marg column has sparsity (missing) of 0.96125, missing percentage above threshold of 0.95, hence skipped \u001b[0m\n",
      "\u001b[93m specify_histology_if_non_o column has sparsity (missing) of 0.965, missing percentage above threshold of 0.95, hence skipped \u001b[0m\n",
      "\u001b[93m bra_cup_size column has sparsity (missing) of 0.98125, missing percentage above threshold of 0.95, hence skipped \u001b[0m\n",
      "\u001b[93m bra_cup_size_measure column has sparsity (missing) of 0.98125, missing percentage above threshold of 0.95, hence skipped \u001b[0m\n",
      "\u001b[93m lymph_node_max_size_mm0 column has sparsity (missing) of 0.985, missing percentage above threshold of 0.95, hence skipped \u001b[0m\n",
      "\u001b[93m internal_mammary_lymph_nod column has sparsity (missing) of 0.99125, missing percentage above threshold of 0.95, hence skipped \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# AutoML on y column to perform imputation\n",
    "# Sort the columns with ascending order of sparsity\n",
    "columns = sorted(df.columns, key=lambda x: df[x].isna().sum())\n",
    "sparsity_threshold = 0.95 # If more than sparsity_threshold of cells are missing, then don't impute the column\n",
    "# Remove bad columns\n",
    "for y_col in list(columns):\n",
    "    if y_col not in pre_cols:\n",
    "        continue\n",
    "    if y_col == \"did_the_patient_receive_pm\":\n",
    "        my_print(f\"{y_col} column is the target column, hence skipped.\")\n",
    "        columns.remove(y_col)\n",
    "    elif col_types[y_col] == 'time':\n",
    "        my_print(f\"{y_col} column has type Time, hence skipped.\")\n",
    "        columns.remove(y_col)\n",
    "    elif df[y_col].isna().sum() == 0:\n",
    "        my_print(f\"{y_col} column has no missing values, hence skipped.\")\n",
    "        columns.remove(y_col)\n",
    "    elif df[y_col].isna().sum() / len(df) > sparsity_threshold:\n",
    "        my_print(f\"{y_col} column has sparsity (missing) of {df[y_col].isna().sum() / len(df)}, missing percentage above threshold of {sparsity_threshold}, hence skipped\")\n",
    "        columns.remove(y_col)\n",
    "# TODO: check why we need to convert Time to Real types? Eg dob, ...\n",
    "col_types = dict(map(lambda x: (x[0], x[1].replace(\"time\", \"real\")), col_types.items()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2bf761f",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Iteratively impute missing values in the DataFrame\n",
    "# Note: in production settings (real-world), we have only PRE column data\n",
    "#       therefore, we will need to infer/predict INTRA and POST given just PRE\n",
    "\n",
    "output_dir = r\"D:\\YifuChen\\Jan20-2022\\H2ODAI\\dai-1.10.1\\data\\Imputed\"\n",
    "pickle_dir = r\"D:\\YifuChen\\Jan20-2022\\H2ODAI\\dai-1.10.1\\data\\Cache\"\n",
    "\n",
    "debug_mode = False # If True, don't cactch Errors and reduce training time allowed\n",
    "use_PRE_only = True # If True, then use only PRE to infer POST and INTRA \n",
    "skip_nonimportant = False # If True, then skip non-important columns\n",
    "num_col_models = 2 # TODO: The number of models used to infer missing values of a column. \n",
    "                   # Eg if set to 2, first model trains on the first half of data and infers the second half, and so on\n",
    "num_epochs = 20\n",
    "# max_runtime_secs is a hyperparameter for time allowance, set it to > 100 for real-world settings\n",
    "if debug_mode:\n",
    "    max_runtime_secs = 2\n",
    "else:\n",
    "    max_runtime_secs = 180\n",
    "\n",
    "my_print(f\"There are {len(df) * len(df.columns)} cells in total ({len(df)} x {len(df.columns)}).\", add_sep=True)    \n",
    "\n",
    "# Record Missing Cells that should be imputed\n",
    "for row_idx, row in df.iterrows():\n",
    "    for y_col in row.keys():\n",
    "        value = df[y_col][row_idx]\n",
    "        if value == None or str(value) == \"nan\" or (use_PRE_only and (y_col in intra_cols+post_cols)):\n",
    "            missing_cells[(row_idx, y_col)] = True\n",
    "        else:\n",
    "            missing_cells[(row_idx, y_col)] = False\n",
    "my_print(f\"There are {sum(missing_cells.values())} missing/POST cells that will need to be imputed.\", add_sep=True)\n",
    "\n",
    "if use_checkpoint:\n",
    "    df = pd.read_csv(df_checkpoint_path)\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop('Unnamed: 0', 1)\n",
    "    print(\"Using a previously imputed csv as starting point:\", df_checkpoint_path)\n",
    "else:\n",
    "    None\n",
    "# Start Imputing\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch % 2:\n",
    "        time.sleep(300) # Sleep five minutes to cool the CPU and other hardware, for every two epochs\n",
    "    # Given a dataframe, split into train and test\n",
    "    # And train models to predict each column given all other columns\n",
    "    # (Helpful for iterative imputation of missing values)\n",
    "    df_h2o1 = H2OFrame(df.iloc[:len(df_all)//2,:], column_types=col_types)\n",
    "    df_h2o2 = H2OFrame(df.iloc[len(df_all)//2:,:], column_types=col_types)\n",
    "    # TODO: implement train-test split in production settings to evaluate performance\n",
    "    # train, test = df_h2o.split_frame(ratios=[1.0])\n",
    "    train1 = df_h2o1\n",
    "    train2 = df_h2o2\n",
    "    my_print(f\"\\nEpoch {epoch}...\", add_sep=True)\n",
    "    for i in range(len(columns)):\n",
    "        if not debug_mode:\n",
    "            time.sleep(60) # Sleep one minute to cool the CPU and other hardware, for every column\n",
    "        y_col = columns[i]\n",
    "        col_models = {}\n",
    "        sparsity = df[y_col].isna().sum() / len(df)\n",
    "        my_print(f\"Processing {i}/{len(columns)}, {y_col} (sparsity {sparsity}), index in original DF = {list(df.columns).index(y_col)}\", add_sep=True)\n",
    "        if y_col in pre_cols and sparsity <= 0.001:\n",
    "            my_print(f\"Skipped {y_col} column because it has no missing values\")\n",
    "            continue\n",
    "        elif y_col not in important_cols and skip_nonimportant:\n",
    "            my_print(f\"Skipped {y_col} column because it's not among the {len(important_cols)} important columns:\\n{important_cols}\\n\")\n",
    "            continue\n",
    "        try:\n",
    "            my_print(f\"\\nTraining AutoML to predict {y_col} of type {df[y_col].describe().dtype}\")\n",
    "        except:\n",
    "            my_print(f\"\\nTraining AutoML to predict {y_col}\")\n",
    "        try:\n",
    "            # Train the model\n",
    "            model1 = H2OAutoML(max_runtime_secs=max_runtime_secs,seed=1)\n",
    "            model2 = H2OAutoML(max_runtime_secs=max_runtime_secs,seed=2)\n",
    "    #         col_models[y_col] = (model1, model2)\n",
    "            if y_col in imputation_dict:\n",
    "                None\n",
    "            elif y_col not in models or models[y_col] == None:\n",
    "                if use_PRE_only:\n",
    "                    x_cols = list(pre_cols)\n",
    "                else:\n",
    "                    x_cols = list(columns)\n",
    "                if y_col in x_cols:\n",
    "                        x_cols.remove(y_col)\n",
    "    #             (model1, model2) = col_models[y_col]\n",
    "                model1.train(x=x_cols, y=y_col, training_frame=train1)\n",
    "                model2.train(x=x_cols, y=y_col, training_frame=train2)\n",
    "                if use_PRE_only:\n",
    "                    models_pre_only[y_col] = (model1, model2) \n",
    "                else:\n",
    "                    models[y_col] = (model1, model2)\n",
    "            else:\n",
    "                if use_PRE_only:\n",
    "                    (model1, model2) = models_pre_only[y_col]\n",
    "                else:\n",
    "                    (model1, model2) = models[y_col]\n",
    "            # Print leaderboard metrics for this column\n",
    "            leadbrd1 = model1.leaderboard.head(rows=model1.leaderboard.nrows)\n",
    "            leadbrd2 = model2.leaderboard.head(rows=model2.leaderboard.nrows)\n",
    "            print(leadbrd1.head())\n",
    "            print(leadbrd2.head())\n",
    "            leaderboards[(epoch, y_col)] = (leadbrd1.as_data_frame(), leadbrd2.as_data_frame())\n",
    "            # Fill in the missing cells by using the trained models for each column\n",
    "            temp1 = pd.DataFrame(df.loc[:len(df)//2-1, df.columns != y_col]) # Get all other columns except y_col, first half\n",
    "            temp2 = pd.DataFrame(df.loc[len(df)//2:, df.columns != y_col]) # Get all other columns except y_col, second half\n",
    "            temp_col_types = {k: col_types[k] for k in set(col_types.keys()) - set([y_col])}\n",
    "            temp_h2o_frame1 = H2OFrame(temp1, column_types=temp_col_types)\n",
    "            temp_h2o_frame2 = H2OFrame(temp2, column_types=temp_col_types)\n",
    "            pred2 = model1.leader.predict(temp_h2o_frame2).as_data_frame()[\"predict\"] # Use first half to impute second half\n",
    "            pred1 = model2.leader.predict(temp_h2o_frame1).as_data_frame()[\"predict\"] # Use second half to impute first half\n",
    "            preds = pd.concat([pred1, pred2], ignore_index=True)\n",
    "            cache[epoch][y_col] = preds\n",
    "            assert len(preds) == len(df)\n",
    "            for row_idx, row in df.iterrows():\n",
    "                if missing_cells[(row_idx, y_col)] == True:\n",
    "                    if y_col in imputation_dict:\n",
    "                        df[y_col][row_idx] = imputation_dict[y_col]\n",
    "                    else:\n",
    "                        df[y_col][row_idx] = preds[row_idx]\n",
    "                    num_filled += 1\n",
    "            # Save intermediate results\n",
    "            my_print(f\"Finished imputing column {y_col}, {num_filled} cells have been filled so far.\")\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%Y-%m-%d-%H_%M_%S\")\n",
    "            temp_path = os.path.join(output_dir, fr\"ALLTranImputed_epoch{epoch}_{max_runtime_secs}sec_{'PREonly' if use_PRE_only else 'All'}_{str(i)}_{str(current_time)}.csv\")\n",
    "            df.to_csv(temp_path)\n",
    "            my_print(f\"Saved imputed data to csv at {temp_path}.\")\n",
    "            pickle_path_cache = os.path.join(pickle_dir, fr\"cache_epoch{epoch}_{max_runtime_secs}sec_{'PREonly' if use_PRE_only else 'All'}_{str(i)}_{str(current_time)}.pkl\")\n",
    "            pickle_path_leader = os.path.join(pickle_dir, fr\"leader_epoch{epoch}_{max_runtime_secs}sec_{'PREonly' if use_PRE_only else 'All'}_{str(i)}_{str(current_time)}.pkl\")\n",
    "            with open(pickle_path_cache, 'wb') as handle:\n",
    "                pickle.dump(cache, handle)\n",
    "                my_print(f\"Saved cache to pickle file at {pickle_path_cache}.\")\n",
    "            with open(pickle_path_leader, 'wb') as handle:\n",
    "                pickle.dump(leaderboards, handle)\n",
    "                my_print(f\"Saved leaderboards to pickle file at {pickle_path_leader}.\")\n",
    "            # Restart H2O cluster to prevent disconnections\n",
    "        except Exception as e:\n",
    "    #         if debug_mode:\n",
    "            my_print(f\"\\nSkipped training for \", y_col, f\"due to an error.{len(skipped_cols)} / {len(columns)} have been skipped so far.\\nError Message: {str(e)}\", add_sep=True)\n",
    "            skipped_cols.add(y_col)\n",
    "            models[y_col] = None\n",
    "            \n",
    "print(\"Imputation is complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6028c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## Iteratively impute missing values in the DataFrame\n",
    "# # Note: in production settings (real-world), we have only PRE column data\n",
    "# #       therefore, we will need to infer/predict INTRA and POST given just PRE\n",
    "\n",
    "# output_dir = r\"D:\\YifuChen\\Jan20-2022\\H2ODAI\\dai-1.10.1\\data\\Imputed\"\n",
    "# pickle_dir = r\"D:\\YifuChen\\Jan20-2022\\H2ODAI\\dai-1.10.1\\data\\Cache\"\n",
    "\n",
    "# debug_mode = True # If True, don't cactch Errors and reduce training time allowed\n",
    "# use_PRE_only = True # If True, then use only PRE to infer POST and INTRA \n",
    "# skip_nonimportant = False # If True, then skip non-important columns\n",
    "# num_col_models = 2 # TODO: The number of models used to infer missing values of a column. \n",
    "#                    # Eg if set to 2, first model trains on the first half of data and infers the second half, and so on\n",
    "# # max_runtime_secs is a hyperparameter for time allowance, set it to > 100 for real-world settings\n",
    "# if debug_mode:\n",
    "#     max_runtime_secs = 2\n",
    "# else:\n",
    "#     max_runtime_secs = 180\n",
    "\n",
    "\n",
    "# for i in range(len(columns)):\n",
    "#     y_col = columns[i]\n",
    "#     col_models = {}\n",
    "#     sparsity = df[y_col].isna().sum() / len(df)\n",
    "#     my_print(f\"Processing column {i}/{len(columns)} (sparsity {sparsity}), the column name is {y_col}, its index in the original DF is {list(df.columns).index(y_col)}\")\n",
    "#     if y_col in pre_cols and sparsity <= 0.001:\n",
    "#         my_print(f\"Skipped {y_col} column because it has no missing values\")\n",
    "#         continue\n",
    "#     elif y_col not in important_cols and skip_nonimportant:\n",
    "#         my_print(f\"Skipped {y_col} column because it's not among the {len(important_cols)} important columns:\\n{important_cols}\\n\")\n",
    "#         continue\n",
    "#     try:\n",
    "#         my_print(f\"\\nTraining AutoML to predict {y_col} of type {df[y_col].describe().dtype}\")\n",
    "#     except:\n",
    "#         my_print(f\"\\nTraining AutoML to predict {y_col}\")\n",
    "#     try:\n",
    "#         # Train the model\n",
    "#         model1 = H2OAutoML(max_runtime_secs=max_runtime_secs,seed=1)\n",
    "#         model2 = H2OAutoML(max_runtime_secs=max_runtime_secs,seed=2)\n",
    "# #         col_models[y_col] = (model1, model2)\n",
    "#         if y_col in imputation_dict:\n",
    "#             None\n",
    "#         elif y_col not in models or models[y_col] == None:\n",
    "#             if use_PRE_only:\n",
    "#                 x_cols = list(pre_cols)\n",
    "#             else:\n",
    "#                 x_cols = list(columns)\n",
    "#             if y_col in x_cols:\n",
    "#                     x_cols.remove(y_col)\n",
    "# #             (model1, model2) = col_models[y_col]\n",
    "#             model1.train(x=x_cols, y=y_col, training_frame=train1)\n",
    "#             model2.train(x=x_cols, y=y_col, training_frame=train2)\n",
    "#             if use_PRE_only:\n",
    "#                 models_pre_only[y_col] = (model1, model2) \n",
    "#             else:\n",
    "#                 models[y_col] = (model1, model2)\n",
    "#         else:\n",
    "#             if use_PRE_only:\n",
    "#                 (model1, model2) = models_pre_only[y_col]\n",
    "#             else:\n",
    "#                 (model1, model2) = models[y_col]\n",
    "#         # Print leaderboard metrics for this column\n",
    "#         leadbrd1 = model1.leaderboard.head(rows=model1.leaderboard.nrows)\n",
    "#         leadbrd2 = model2.leaderboard.head(rows=model2.leaderboard.nrows)\n",
    "#         print(leadbrd1.head())\n",
    "#         print(leadbrd2.head())\n",
    "#         leaderboards[y_col] = (leadbrd1, leadbrd2)\n",
    "#         # Fill in the missing cells by using the trained models for each column\n",
    "#         for row_idx in range(len(df)):\n",
    "#             if row_idx > 5 and debug_mode: # In debug mode, skip rows other than the first five\n",
    "#                 continue\n",
    "#             value = df[y_col][row_idx]\n",
    "#             # Impute and fill in the cell if the value is missing, or if there exists an value but the column is INTRA/POST\n",
    "#             if value is None or str(value) == \"nan\" or (use_PRE_only and (y_col in intra_cols or y_col in post_cols)):\n",
    "#                 if y_col not in imputation_dict:\n",
    "#                     temp = pd.DataFrame(df.loc[row_idx, df.columns != y_col]).T # Get all other columns except y_col\n",
    "#                     temp_col_types = {k: col_types[k] for k in set(col_types.keys()) - set([y_col])}\n",
    "#                     temp_h2o_frame = H2OFrame(temp, column_types=temp_col_types)\n",
    "#                     try:\n",
    "#                         # Predict using the model trained on other half of the data, so that it's unseen before\n",
    "#                         if row_idx >= len(df)//2:\n",
    "#                             pred = model1.leader.predict(temp_h2o_frame)[\"predict\"]\n",
    "#                         else:\n",
    "#                             pred = model2.leader.predict(temp_h2o_frame)[\"predict\"]\n",
    "#                         assert pred.max() == pred.min() # Check that there is only one prediction\n",
    "#                         pred =  pred.min()\n",
    "#                         cache[y_col][row_idx] = pred\n",
    "#                         df[y_col][row_idx] = pred # Write the imputed value; Note: this is correct, do not use loc\n",
    "#                         num_filled += 1 \n",
    "#                         my_print(f\"Column `{str(y_col)}`was {value} at Row {row_idx}, hence filling via imputed value {pred}. {num_filled} cells have been imputed so far.\")\n",
    "#                     except Exception as e:\n",
    "#                         if debug_mode:\n",
    "#                             raise e\n",
    "#                         my_print(\"Couldn't infer\", y_col, str(e))\n",
    "#                 else:\n",
    "#                     df[y_col][row_idx] = imputation_dict[y_col]\n",
    "\n",
    "#         # Save intermediate results\n",
    "#         print(f\"Finished imputing column {y_col}\")\n",
    "#         now = datetime.now()\n",
    "#         current_time = now.strftime(\"%Y-%m-%d-%H_%M_%S\")\n",
    "#         temp_path = os.path.join(output_dir, fr\"ALLTranImputed__{max_runtime_secs}sec_{'PREonly' if use_PRE_only else 'All'}_{str(i)}_{str(current_time)}.csv\")\n",
    "#         df.to_csv(temp_path)\n",
    "#         my_print(f\"Saved imputed data to csv at {temp_path}.\")\n",
    "#         pickle_path = os.path.join(pickle_dir, fr\"cache_{max_runtime_secs}sec_{'PREonly' if use_PRE_only else 'All'}_{str(i)}_{str(current_time)}.pkl\")\n",
    "#         with open(pickle_path, 'wb') as handle:\n",
    "#             pickle.dump(cache, handle)\n",
    "#             my_print(f\"Saved cache to pickle file at {pickle_path}.\")\n",
    "#         # Restart H2O cluster to prevent disconnections\n",
    "#     except Exception as e:\n",
    "# #         if debug_mode:\n",
    "#         print(str(e))\n",
    "#         my_print(f\"\\nSkipped training for \", y_col, f\"due to an error.{len(skipped_cols)} / {i+1} have been skipped so far.\", str(e), \"\\n\")\n",
    "#         skipped_cols.append(y_col)\n",
    "#         models[y_col] = None\n",
    "# print(\"Imputation is complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db53e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(df)\n",
    "for col in post_cols + intra_cols:\n",
    "    if col == \"did_the_patient_receive_pm\":\n",
    "        continue\n",
    "    if skip_nonimportant and col in important_cols and df[col].isna().sum() == 0:\n",
    "        continue\n",
    "    if df[col].isna().sum() / len(df) > sparsity_threshold:\n",
    "        # Drop sparse columns in Intra and Post\n",
    "        df2 = df2.drop(col, 1)\n",
    "\n",
    "bmi = []\n",
    "for i, row in df2.iterrows():\n",
    "    temp = row[\"weight_kg\"] / ((row[\"height_cm\"] / 100) ** 2)\n",
    "    bmi.append(temp)\n",
    "df2.insert(21, \"bmi\", bmi)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d-%H_%M_%S\")\n",
    "temp_path = os.path.join(output_dir, fr\"AllImputed_PRE_{max_runtime_secs}sec_{'PREonly' if use_PRE_only else 'All'}_{str(i)}_{str(current_time)}.csv\")\n",
    "df2.to_csv(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd2872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Fill in the missing cells by using the trained models for each column\n",
    "# df = train.as_data_frame(use_pandas=True)\n",
    "# df_filled = pd.DataFrame(df)\n",
    "# num_filled = 0\n",
    "\n",
    "# from collections import defaultdict\n",
    "# cache = defaultdict(dict)\n",
    "\n",
    "# for row_idx, row in df.iterrows():\n",
    "#     for col_idx, (col, value) in enumerate(row.items()):\n",
    "# #         if col != \"age_at_surg\":b\n",
    "# #         print(str(value))\n",
    "#         if value is None or str(value) == \"nan\":\n",
    "#             my_print(str(col), f\"is None at Row {row_idx}, hence filling\")\n",
    "#             model = models[col]\n",
    "#             temp = pd.DataFrame(row).T\n",
    "#             row_h2o = H2OFrame(temp, column_types=col_types) # Convert to H2O frame for prediction\n",
    "# #             for col in row_h2o.columns:\n",
    "# #                 if str(train[col].dtype) == '<U0' or col in [\"size_of_the_largest_foci_c\"]: # If categorical\n",
    "# #                     print(\"col is categorical:\", col)\n",
    "# #                     row_h2o[col] = row_h2o[col].asfactor()\n",
    "# #                 if col in [\"dob\", \"dximg_date\", \"microcalcifications\", \"prominent_axillary_lymph\"]:\n",
    "# #                     row_h2o[col] = row_h2o[col].asnumeric()\n",
    "#             try:\n",
    "#                 pred = model.predict(row_h2o)\n",
    "#                 cache[col][row_idx] = pred\n",
    "#                 df_filled[col][row_idx] = pred # Note: do not use .loc here\n",
    "#                 num_filled += 1\n",
    "#             except Exception as e:\n",
    "#                 my_print(\"Couldn't infer\", col, \"for row\", str(row_idx), str(e))\n",
    "#         else:\n",
    "#             df_filled[col][row_idx] = value\n",
    "# my_print(f\"Filled in {num_filled} missing values in total using iterative imputation.\")\n",
    "# df.to_csv(r\"D:\\YifuChen\\Jan20-2022\\H2ODAI\\dai-1.10.1\\data\\ALLTran_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae6358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_filled\n",
    "# experiment = {}\n",
    "# max_runtime_secs=100\n",
    "# columns = list(df_all.columns)\n",
    "# # for i, col in enumerate(columns):\n",
    "\n",
    "# res = train_models_from_df(df_all, y_col=col, max_runtime_secs=max_runtime_secs)\n",
    "# experiment[col] = {}\n",
    "# experiment[col][\"train_data\"] = res[\"train\"]\n",
    "# experiment[col][\"test_data\"] = res[\"test\"]\n",
    "# experiment[col][\"models\"] = res[\"models\"]\n",
    "# #     if i > 10:\n",
    "# #         break\n",
    "\n",
    "# # TODO:\n",
    "# # First, impute the PRE columns using PRE columns, fill in missing PRE cells\n",
    "# # Second, impute the INTRA columns using PRE and INTRA columns, fill in missing INTRA cells\n",
    "# # Third, impute the POST columns using PRE, INTRA, and POST columns, fill in missing POST cells\n",
    "\n",
    "# # for df, col_type in zip([df_pre, df_intra, df_post], [\"PRE\", \"INTRA\", \"POST\"]):\n",
    "# #     columns = list(df.columns)\n",
    "# #     for col in columns:\n",
    "# #         if col in \n",
    "# #         res = train_models_from_df(df, y_col=col, max_runtime_secs=max_runtime_secs)\n",
    "# #         experiment[col_type] = {}\n",
    "# #         experiment[col_type][\"train_data\"] = res[\"train\"]\n",
    "# #         experiment[col_type][\"test_data\"] = res[\"test\"]\n",
    "# #         experiment[col_type][\"models\"] = res[\"models\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bf1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a13a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
